<!DOCTYPE html>
<html>

<head>
    <title>ML cheat sheet</title>
    <meta charset="UTF-8">
    <link rel="stylesheet" href='resources/bootstrap.min.css'>
    <link rel="stylesheet" href='resources/styles.css'>
    <script src='resources/html5.js'></script>
    <script src='resources/respond.min.js'></script>
</head>

<body>
    <div class="container-fluid">
        <div class="row">
            <div class="col-sm-12">
                <p>
                    <b>
                        set up issues:
                    </b>
                </p>
                <p>
                    increase number of rows displayed:<br>
                    pd.set_option("display.max_rows", 300)
                </p>
                <p>
                    switch off warning messages:<br>
                    import warnings<br>
                    warnings.simplefilter(action='ignore')
                </p>
                <p>
                    <b>
                        gather data:
                    </b>
                </p>
                <pre>
Retrieve and extract tar-file:
import os
import tarfile
datapath = "/User/me/datasets"
dataurl = "https://raw/githubusercontent.com/.../datasets/abc.tgz"
urllib.request.urlretrieve(dataurl, os.path.join(datapath, "thedata.tgz"))
data_tgz = tarfile.open(os.path.join(datapath, "thedata.tgz"))
data_tgz.extractall(path=datapath)
data_tgz.close()
</pre>
                <pre>
Read csv-data in a dataframe:
csv_path = os.path.join(datapath, "thedata.csv")
dataframe_ = pd.read_csv(csv_path)
</pre>
                <p>
                    <b>
                        screen data:
                    </b>
                </p>
                <pre>
basic data type information about a data frame:
dataframe_.info()
</pre>
                <pre>
basic statistics about a dataframe:
dataframe_.describe()
</pre>
                <pre>
distributions of data of all columns of a dataframe:
%matplotlib inline
import matplotlib.pyplot as plt
binning_size = 50
dataframe_.hist(bins=binning_size, figsize=20,15))
</pre>
                <p>
                    <b>
                        stratify data:
                    </b>
                </p>
                <pre>
create stratum category, with cut-off value:
divisor = value_at_cut_off_point / number_of_categories
dataframe_["stratum_cat"] = np.ceil(dataframe_["important_predictor_cont"]/divisor)
dataframe_["stratum_cat"].where(dataframe_["stratum_cat"] < number_of_categories, number_of_categories, inplace=True)
</pre>
                <p>
                    <b>
                        split into test and training set:
                    </b>
                </p>
                <pre>
non-stratified random splitting into test and traning set, using permutations:
test_ratio = 0.2
def split_train_test(data, test_ratio):
    shuffled_indices = numpy.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices],data.iloc[test_indices]
train_set, test_set = split_train_test(dataframe_, 0.2)
</pre>
                <pre>
quasi-randomized selection by hashing an id-column:
import hashlib
def test_set_check(identifier, test_ratio, hash):
    return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio
def split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_:
                    test_set_check(id_, test_ratio, hash))
    return data.loc[~in_test_set],data.loc[in_test_set]
df_with_id = dataframe_.reset_index() # adds an index-column
train_set, test_set = split_train_test_by_id(df_with_id, 0.2, "index")
</pre>
                <pre>
use train_test_split from scikitlearn, with fixed random seed:
from skearn.model_selection import train_test_split
train_set, test_set = train_test_split(dataframe_, test_size=0.2, random_state=42)
</pre>

                <pre>
stratified splitting into test and training set:
from sklearn.model_selection import StratifiedShuffleSplit
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(dataframe_, dataframe_["stratum_cat"]):
    strat_train_set = dataframe_.loc[train_index]
    strat_test_set = dataframe_.loc[test_index]
</pre>

                <p>
                    <b>
                        explore data:
                    </b>
                </p>
                <pre>
scatter-plot for specific columns:
dataframe_.plot(kind="scatter", x="col1", y="col2")
</pre>

                <pre>
produce the (pair-wise) correlation matrix:
corr_matrix = dataframe_.corr()
</pre>

                <pre>
produce the scatter-matrix:
from pandas.plotting import scatter_matrix
attributes = ["col1", "col2", "col3", "col4"]
scatter_matrix(dataframe_[attributes], figsize=(12, 8))
</pre>
                <p>
                    <b>
                        clean data:
                    </b>
                </p>
                <pre>
drop a column from a dataframe (inplace):
dataframe_.drop('colname1', 1, inplace=True)
</pre>

                <pre>
replace 0's with nan in a column (inplace):
dataframe_['colname1'].replace(0, numpy.nan, inplace=True)
</pre>
                <pre>
replace missing values with central tendency values:
# select the numerical columns from the dataframe (alternatively: drop the non-numerical columns):
dataframe_num = dataframe[['numcol1','numcol2',...,'numcoln']] 
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="median")
imputer.fit(dataframe_num)
X = imputer.transform(dataframe_num)
# transform the numpy-array back to a dataframe:
dataframe_tr = pd.DataFrame(X, columns = dataframe_num.columns)
</pre>
                <p>
                    <b>
                        transform data:
                    </b>
                </p>
                <pre>
convert categorical variables into numerical variables:
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
dataframe_cat_encoded = encoder.fit_transform(dataframe_['categorical_column'])
</pre>
                <pre>
applying a one-hot-encoder (results in a SciPy sparse matrix):
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(categories='auto')
dataframe_cat_1hot = encoder.fit_transform(dataframe_cat_encoded.reshape(-1,1))
</pre>
                <pre>
directly convert from categorical (result in an array, NOT in a sparse matrix):
from sklearn.preprocessing import LabelBinarizer
encoder = LabelBinarizer()
dataframe_cat_1hot = encoder.fit_transform(dataframe_['categorical_column'])
</pre>
                <pre>
it might be that the Standard Label Binarizer does not work with FeatureUnion (combined pipeline of numerical and 
categorical data processing), in that case a customized variant need to be used:
from sklearn.base import TransformerMixin
class MyLabelBinarizer(TransformerMixin):
    def __init__(self, *args, **kwargs):
        self.encoder = LabelBinarizer(*args, **kwargs)
    def fit(self, x, y=0):
        self.encoder.fit(x)
        return self
    def transform(self, x, y=0):
        return self.encoder.transform(x)    
</pre>
                <pre>
create additional calculated columns (as a numpy array) from a dataframe:
from sklearn.base import BaseEstimator, TransformerMixin
attr1, attr2, attr3 = 3,6,7
class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def transform(self, X, y=None):
        new_attr1 = X[:, attr1] / X [:, attr3]
        new_attr2 = X[:, attr2] / X[:, attr3]
        return np.c_[X, new_attr1, new_attr2]
attr_adder = CombinedAttributesAdder()
array_extra_attribs = attr_adder.transform(dataframe_.values)
</pre>
                <p>
                    <b>
                        pipelining:
                    </b>
                </p>
                <pre>
example data pipeline that uses a FeatureUnion:
num_attribs = list(dataframe_num)
cat_attribs = ["catattr1","catattr2"]
num_pipeline = Pipeline([
    ('selector', DataFrameSelector(num_attribs)), 
    ('imputer', Imputer(strategy="median")), 
    ('attribs_adder', CombinedAttributesAdder()), 
    ('std_scaler', StandardScaler()),
])
cat_pipeline = Pipeline([
    ('selector', DataFrameSelector(cat_attribs)), 
    ('label_binarizer', LabelBinarizer()),
])
from sklearn.pipeline import FeatureUnion
full_pipeline = FeatureUnion(transformer_list) = [
    ("num_pipeline", num_pipeline),
    ("cat_pipeline", cat_pipeline),
])
</pre>
                <p>
                    <b>
                        some quick & dirty examples of model-fitting:
                    </b>
                </p>
                <p>
                    <i>
                        k-nearest neighbors, example 1:
                    </i>
                </p>
                <pre>
# using scikitlearn k-nearest neighbors
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
from pylab import rcParams

from sklearn import neighbors
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn import metrics

np.set_printoptions(precision=4, suppress=True) 
rcParams['figure.figsize'] = 7, 4
plt.style.use('seaborn-whitegrid')

address = 'mtcars.csv'
cars = pd.read_csv(address)
cars.columns = ['car_names','mpg','cyl','disp', 'hp', 'drat', 'wt', 'qsec', 'vs', 'am', 'gear', 'carb']

X_prime = cars[['mpg','disp','hp','wt']]
y = cars[['am']]

X = preprocessing.scale(X_prime)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=17)

# if no value for param n_neighbors is supplied, the default of 5 is taken
clf = neighbors.KNeighborsClassifier()
clf.fit(X_train, y_train.values.ravel())

y_expect = y_test
y_pred = clf.predict(X_test)

print(metrics.classification_report(y_expect, y_pred))
</pre>
                <p>
                    <i>
                        k-nearest neighbors, example 2:
                    </i>
                </p>
                <pre>
import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
ccdefault = pd.read_csv('D:\studium\Systematisches Studium - Statistik, Kybernetik, Thermodynamik\Python allgemein\CodeRepository\other data files\ccdefault.csv')
ccd = ccdefault[ccdefault.columns[1:]]
test_idx = np.random.uniform(0, 1, len(ccd)) <= .333
train = ccd[test_idx == True]
test = ccd[test_idx == False]
features = [ 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'DEFAULT' ]
clf = KNeighborsClassifier(n_neighbors = 5)
clf.fit(train[features], train['DEFAULT'])
preds = clf.predict(test[features])
accuracy = np.where(preds == test['DEFAULT'], 1, 0).sum() / float(len(test))*100
print ("Neighbors: %d, Accuracy: %2d%%" % (n, round(accuracy, 2)))
</pre>
                <p>
                    <b>
                        Helper classes and functions:
                    </b>
                </p>
                <pre>
# feed the numerical columns of a dataframe into a pipeline:
from sklearn.base import BaseEstimator, TransformerMixin
class DataFrameSelector(BaseEstimator, TransformerMixin): 
    def __init__(self, attribute_names):
        self.attribute_names = attribute_names 
    def fit(self, X, y=None):
        return self
def transform(self, X):
    return X[self.attribute_names].values
</pre>
                <pre>
# implementation of cross-validation:
classifier = SomeFancyClassifier()

from sklearn.model_selection import StratifiedKFold 
from sklearn.base import clone
skfolds = StratifiedKFold(n_splits=5, random_state=123)

for train_index, test_index in skfolds.split(X_train, y_train): 
    clone_classifier = clone(classifier)
    X_train_folds = X_train[train_index]
    y_train_folds = (y_train[train_index])
    X_test_fold = X_train[test_index] y_test_fold = (y_train[test_index])
    clone_clf.fit(X_train_folds, y_train_folds)
    y_pred = clone_clf.predict(X_test_fold)
    n_correct = sum(y_pred == y_test_fold)
    print(n_correct / len(y_pred))
</pre>
                <p>
                    <b>
                        Classification:
                    </b>
                </p>
                <pre>
# framing a classification problem for a binary classifier (ex.: Gradient Descent Classifier) and training it
X_train, X_test, y_train, y_test = X[:training_size], X[training_size:], y[:training_size], y[training_size:]
y_train_singleclass = (y_train == "some_value") # true for all instances of some_value, else False
y_test_singleclass = (y_test == "some_value")
from sklearn.linear_model import SGDClassifier
sgd_clf = SGDClassifier(random_state=123)
sgd_clf.fit(X_train, y_train_singleclass)
</pre>
                <pre>
# applying some performance measures on the result:
from sklearn.model_selection import cross_val_score
accuracy_score = cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")

from sklearn.model_selection import cross_val_predict
y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_singleclass, cv=3)
from sklearn.metrics import confusion_matrix
confusion_matrix(y_train_singleclass, y_train_pred)

from sklearn_metrics import precision_score, recall_score, f1_score
precision_score(y_train_singleclass, y_train_pred)
# same with recall_score, f1_score
</pre>
                <pre>
# force one-vs-one classifier on binary classififer that would use ove-vs-all by default
from sklearn.multiclass import OneVsOneClassifier
ovo_clf = OneVsOneClassifier(SGDClassifier(random_state=123))
ovo_clf.fit(X_train, y_train)
</pre>
</body>

</html>
