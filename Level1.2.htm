<html>
<head>
<meta http-equiv=Content-Type content="text/html; charset=UTF-8">
<link rel="stylesheet" type="text/css" href="css/map_css.css"> 
</head>
<body>
<div class=margin_>
&nbsp;
<h2>Article collection "Cybernetic philosophy"</h2>
<br>
<p class=PGStandard><span lang=EN-GB>Level 1.2</span></p>
<h3 align='center'>The interlocking of problems and purposes</h3>
<h3 align='center'>or</h3>
<h3 align='center'>Solving problems, Reaching Goals, Growing</h3>
<br>
<p class=PGStandard style='text-align:justify'><span lang=EN-GB>
If problems are addressed by a
cybernetic agent, then their solution serves a certain purpose and their level of
complexity is such that the agent's capacities enables it to potentially
solve them sucessfully. For very basic agents, important problems are likely to be
closely connected with self-preservation, which is the most primordial goal that one
can formulate. In cybernetic language, such kinds of problems are disturbances
that threaten the <i>essential variables</i> (see p. 197 in <a
href="#AshbyIntroductionToCybernetics">[1]</a>) of an agent. The agent then has
to take measures like seeking shelter, finding food, fighting against or running away from lethal
threats etc., in order to preserve (in case of anticipatory control) or 
reestablish (in case of error-based control) the state of the essential variables. 
There are other variables whose regulation can be a goal of a goal-driven entity. 
But these variables often depend on the state of the essential variable, and, correspondingly,
the goal of regulating these other variables depends critically on the goal of regulating
the essential variables. Seen in this light, self-preservation can be seen as a particular, 
but also as the most important, example of the cybernetic
problem of <a href='https://en.wikipedia.org/wiki/Regulation' 
target='_blank'>regulation</a>. What complements regulation is <i>growth</i>. It shall
be defined as the process that overcomes a problem, rather than continuously solving
its recurring instances, which is the approach of regulation. When growth has happened
with respect to a certain problem, the corresponding control circuit for that problem can 
be "discharged". In a weaker sense of growth, the regulation of recurring instances of
a problem has been delegated to control circuits that are reliable to such an extent 
and that the agent invokes so naturally that these problems at least do not require
the attention of the agent anymore.
</span></p>
<p class=PGStandard style='text-align:justify'><span lang=EN-GB>
What an essential variable is, or what a variable is whose states define other, 
higher-level goals of a goal-driven entity, depends on the domain that one considers.
Examples of cybernetic entities are machines, especially adaptive machines, artifical
intelligence, enterprises and companies, states, and biological organisms. For
each of these, the respective essential or higher-level variable can be defined.
If one wants to consider an abstract construction of a basic paradigmatic 
agent engaged in goal-oriented activity, one finds an
extended version of Maxwell’s demon as a model of such a paradigmatic agent
(see <a href="Level1.1.2.1.htm">Level 1.1.2.1</a> for a coverage of the dual
evaluability of what the agent does, in informational and in causal language).
Since our world is an entropic one and a world with limited resources, acting
has to be informed, and, on the basic level of self-preservation, turns on
maintaining the entropic level of the agent by collecting tokens of free
energy. The setup that features Maxwell’s demon is a scenario where the free
energy is created by a <i>sorting operation</i>. As the analysis shows, 
performing the sorting is itself dependent on an extrinsic inflow 
of additional free energy necessary to maintain that operation. That is why
Maxwell's demon is such a relevant object of study. It shows that, being
subject to physics as we currently understand it, we cannot escape the precarious situation
that the entropic order of the world imposes on us. </span></p>
<p class=PGStandard style='text-align:justify'><span lang=EN-GB>
Considering the naturally given examples of cybernetic agency, we see that
even the simplest biological and technical 
examples of agency feature one form or another of consumption of free energy 
in order to produce purposive actions, i.e. perform physical work. That being said, 
the intervention in the form of a purposive action should only occur if necessary;
that is, if the considered process does not produce the desired outcome anyway, since
in these cases the expense of free energy can be spared.
It is for those reasons that the creation of what is called a strong artificial intelligence requires at 
an early stage of development endowing one's creation with the capacity of
judging spontaneous and non-spontaneous processes, or in other words
with the capacity of distinguishing between those processes that happen naturally 
and those that require
an intervention by the agent. The statistics of processes is addressed on <a
href="Level1.2.4.htm">Level 1.2.4</a>. From the viewpoint of physics, the mathematics
of entropy is the ultimate cause of why we are dealing with the whole subject area
of cybernetics, since entropy explains the fundamental constraints imposed on the
teleological entities that cybernetics studies. The rest of the sub-levels of 
1.2 are structured as follows. 
<a href="Level1.2.1.htm">Level 1.2.1</a> provides a more
philosophical outlook on cybernetics, and ponders arguments 
for engaging in its study. As mentioned above, human beings are only a subset of
the structures relevant to cybernetics. But when human beings are the ones studying
cybernetics, then the class of human beings is obviously a very interesting area of 
study within cybernetics. Accordingly, the considerations made on Level 1.2.1 are 
explicity made from the human viewpoint, and the emphasis will be on the philosophical
implications of cybernetics. (Other domain-specific aspects of the cybernetics
of human beings concern the human physiology, for example). On that sub-level,
it will be found that we should delegate the 
typical problems like regulation and anticipatory or error based-control
to systems of automation, so that we can address more valuable goals like continuous 
empowerment and intellectual development. <a
href="Level1.2.2.htm">Level 1.2.2</a> considers the most important object
of study of cybernetics, the control circuit. 
It is a domain-independent structure that can be encountered in every one of its sub-branches. 
Finally, <a href="Level1.2.3.htm">Level 1.2.3</a> will be a higher-level description of 
cybernetics that abstracts from mathematical and technical aspects, in contrast to
Level 1.2.2 and 1.2.4. It focuses on the agent as the owner of a more complex
arrangements of goals, and as an invocator of services in order to solve problems,
as well as on the costs of the invoked services and of the opportunities linked 
to the agent's goals.
</span></p>
<p class=PGStandard style='text-align:justify'><span lang=EN-GB>
Turning back to the control-circuit, one can say that its study is what cybernetics is often 
equated with; or, alternatively, with the
study of <i>feedback</i>. 
This is owed to the general importance these ideas have with
respect to actions.
When a cybernetic agent tackles a problem,
it is often required to determine if the goal has been
reached, even if the solution to the problem consists in a purely theoretical
exercise, where, at the end of a process of thinking, of a deduction, of
a computation, it needs to be confirmed whether the result has really
answered the question that had been mooted. This is true even more if
determining whether a goal has been reached is dependent on measuring
contingent information. An agent then has to answer the question “Am I already
there, yes or no?”, and then further adjust its intervention until the question
is answered by “yes”. In other words, the agent's intervention often undergoes several
iterations and follows a cybernetic feedback cycle, or control circuit, 
which <a href="Level1.2.2.htm">Level
1.2.2</a> treats in extact details.
The feedback that closes the said circuit can often be evaluated on two levels:
on the concrete level, when it tells the agent whether it has reached its
concrete goal already, and on a more abstract level that enables the agent to
evaluate whether its intervention is the most adequate one of all possible actions
to be taken in a similar situation. If an agent evaluates the feedback according to the latter sense, then
the agent is called "adaptive".
The study of the adaptive agent concerns the case where statistics are of obvious and vital importance. The statistics
of adaptive systems is dealt with on <a href="Level1.2.2.1.1.htm">Level
1.2.2.1.1</a>. However, even for the non-adaptive agent that pursues a particular goal
in a particular situation statistical problems typically arise, e.g. when the
intelligence concerning its environment is only indicative rather than
decisive, or when its possible actions are only probabilistically effective. 
<a href="Level1.2.2.htm">Level 1.2.2</a> understands the control circuit as probabilistic
from the start (since deterministic circuits are just extreme cases), 
and its statistics are explored in more detail on 
<a href="Level1.2.2.1.htm">Level 1.2.2.1</a>.
As already mentioned, the adaptive agent processes feedback information on 
two levels. It is used for determining whether the concrete goal currently 
dealt with has already been reached, and it is used to optimize behaviour. But
that creates a situation of conflicting goals, because processing information 
always costs resources. Therefore, the adaptive agent is one (important) example 
of a fact that is often encountered in cybernetic studies; it is the fact that a 
cybernetic control unit often has to deal with several control circuits at once, and has
to integrate several data from several levels of representation. Often, conflicting
goals come with these requirements. It is here that we see an aspect of 
<i>requisite variety</i>, one of the measures of an agent's level of <i>empowerment</i> (see p. 202 in <a
href="#AshbyIntroductionToCybernetics">[1]</a>). Whereas variety in acting normally refers to different 
options of acting with respect to one control circuit and therefore to one kind
of problem, one can also apply the idea to the capacity of ranking the relevance
of dealing with conflicting goals, where the best selection
of action consists in judging which control circuit one should pay attention to
in the first place. Another example of this "cross-level" requisite variety in this wider sense of
the word is the abability to perform a drill-down into 
a more detailed level of representation in order to look at what happens exactly during the solution of a 
problem that the agent would usually solve by means of invoking a service (in other
words this capacity consists in being able to perform the drill-down of 
<a href=Level1.htm#topdown>(3)</a> from Level
1). It again turns out to be a question of the agent's requisite
variety that determines if the agent 
could replace the
functional gap by own capacities or alternative services, in case the primary
service fails. Simple examples for this are “backup plans”, safety mechanisms,
or the capacity to do a computation by hand if a computing machine is not at
hand.
</span></p>
<p class=PGStandard style='text-align:justify'><span lang=EN-GB>
The examination of the relation between
different control circuits is vital for the study of cybernetics at a 
more advanced stage, developed beyond the level of being a "flat" study of one single
variable system to be controlled by a control unit. The cross-level study is also
of pivotal importance to the philosophical solution to the problem of existence
offered on <a href="Level1.2.1.htm">Level 1.2.1</a>. It is also a cornerstone
of the definition of what empowerment could mean conceptually. And it gives
rise to the idea of "higher-level" agency, which is examined on 
<a href="Level1.2.3.htm">Level 1.2.3</a>. A high level of agency can mean
that the solution of certain problems is delegated to a lower-level control unit which 
stands in a subservient relation to the high-level agent. The high-level
agent merely invokes the control unit to keep the low-level problems at a distance.
The existence of the low-level control unit enables the high-level agent to address
problems from which it would be excluded otherwise, such that if the control unit
breaks down the agent is forced to address the problems which it is normally not
bothered with. In that, the agent might succeed or not. In the case that it
succeeds, by being able to deploy substitutional services that deal with the problems, 
then one can say that the agent has full requisite variety. Level 1.2.3 will look
at the relations betweens different control circuits and at the problem of 
pursuing different kinds of goals, which can stand in various kinds of relations
to each other.
One example of such a form of higher-level agency is the functional role of management in an
enterprise.
</span></p>
<h4 align='center'>Concluding remark concerning this level</h4>
<p class=PGStandard style='text-align:justify'><span lang=EN-GB>
Whereas Level 1.3 describes the logical
structure that bootstraps this approach to cybernetic questions, and Level 1.1 specifies
its metaphysical basis as a variant of <a href="https://en.wikipedia.org/wiki/Double-aspect_theory"
target='_blank'>double-aspect theory</a> and possibly also a variant of 
<a href="https://en.wikipedia.org/wiki/Voluntarism_(philosophy)" target='_blank'>voluntarism</a>,
this Level 1.2 is the starting point of cybernetics as one would classically construe it, i.e. the structure of regulatory,
or more generally goal-driven structures. The specificiation of Level 1.3 explains the 
reasoning behind dividing the overall subject-matter into the detailed articles of the
sub-levels. On the other hand, without Level 1.1 certain other
questions would remain unanswered, e.g. where the values of the valuation function when
optimizing an action - ultimately - come from. The foundational goal of a cybernetic
entity is self-preservation, and this foundation has to be laid on Level 1.1.,
where it is said that it is the subjective stance that we need to explain why
"I" am different from "her, him, or it", and that "I" should be preserved. That is
the original case of subjective preference. 
When, on Level 1.2 and its sub-levels, the entanglement of purposes and problems, control 
circuits, adaptivity etc. are studied, it is likely that these activities are only ramifications 
of studying the foundational goal of self-preservation. Even the goal
of self-improvement and the corresponding activities of an agent aiming to realise
this goal can be traced back to self-preservation since self-improvement
can be defined as an augmentation of capacities to withstand a wider range of 
disturbances that threaten self-preservation.
Then there are other questions that can be answered
"within" Level 1.2. For example, the question of how a targeted situation can attain
the status of a goal relative to another goal. We often encounter similar relations of 
dependence as between self-preservation and self-improvement, since many other targeted situations stand in
the same ancillary relation to higher-ranking goals, and thereby reach the status of goals
themselves. Examples are 
investments made for future returns (e.g. when a machine is overhauled in order
to increase its life-span), or efforts which are undertaken to gather information that 
the agent needs for future decisions, not immediately providing pay-off, but in some
time (and only contingently) in the future.
</span></p>



<p class=PGStandard><span lang=EN-GB >Further
references (besides the hyperlinked ones):</span></p>

<p class=PGStandard><a name=AshbyIntroductionToCybernetics><span lang=EN-GB
>[1] </span></a><span lang=EN-GB
>W. Ross Ashby: <i>An Introduction to
Cybernetics</i>, Chapman &amp; Hall, 1956</span></p>

<p class=PGStandard><span lang=EN-GB style='color:gray'>&nbsp;</span></p>

</div>

</body>

</html>
