<html>
<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<link rel="stylesheet" type="text/css" href="css/map_css.css"> 
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML" async>
</script>
</head>
<body>
<div class=margin_>
&nbsp;
<h2>Article collection "Cybernetic philosophy"</h2>
<br>
<p class=PGStandard style='text-align:justify'><span lang=EN-GB>Level 1.1.1</span></p>
<h3 align='center'>Agent probabilities</h3>
<h4 align='center'>The difference between interventional and observational statistical data</h4>
<br>
<p class=PGStandard style='text-align:justify'><span lang=EN-GB> 
Accoring to the <a href="https://plato.stanford.edu/entries/physics-Rpcc/" target="_blank">principle of the common cause</a> there are three cases that
could explain an observed correlation of two variables A and B. Either A causes B, or B causes A, or there is a third (and possibly unobserved) variable
C that causes both of them. Represented as a graph, the three cases are thus:</span></p>
<p align="center"><img src="img/common_cause_principle.png" style="max-width:35%;"/></p>
<p class=PGStandard style='text-align:justify'><span lang=EN-GB>
Assuming the perspective of cybernetics, it is the primordial task of an 
agent to control its environment. Observing a correlation between observables is the first step in order to achieve that goal. Thus, if we look at a system of
observables and we have direct access to some of the observables, then we can manipulate other parts of the system via our immediate access to
those directly manipulable variables, given the correlation is of a causal nature and the relation favours such a manipulability. 
The observational data given by merely observing corelations, however, leaves
open which of the three possibilities of a binary correlation is real in a concrete case. Concentrating only on the relation between A and B, and leaving aside other ways to determine
the causal direction between A and B if there is one, then the simplest way to disclose the direction is a direct intervention on both of them and assessing whether 
there is an effect on the correlation. If there is only access to one of them, say A, then manipulating A directly by means of an agent's intervention either
destroyes the correlation or it maintains it. If the latter is the case, then there is a causal relationship between A and B. If the former is the case,
then A is an effect of B or A and B are both effects of a third variable C.
</span></p> 
<p class=PGStandard style='text-align:justify'><span lang=EN-GB>
If the fact of a correlation between the variables is the only information available, then it is crucial that the agents <i>themselves</i> perform the intervention
and do not watch other agents perform it. This would still provide only observational data. But if an agent makes the decision to intervene <i>ex arbitrio</i>, then
the change in A is not caused by anything else in the system except the agent's decision, and any regularly ensuing change in B must be due to A causing B. We 
cannot draw such a conclusion by watching other agents perform the intervention, since their doings could be an effect of a cause that is also a cause of B, 
which renders the relation between A and B unanalyzable. One can therefore say that relying on interventions performed by other agents gives
the correct result barring a false imputation of an agent-status in a possible presence of covert additional causes, while performing the intervention myself
I can be sure to achieve the correct result unless I think of the whole notion
of a free will as something illusory. Without free decisions, there is no agency. Without agency, there is no causation, since if we did not (have to) invertene
into our environment, we would be pure observers, and there would be absolutely no pressure on us to develop a concept of causation. Even if we wanted
to predict what happens in our environment, it would be sufficient to realise that one event temporally follows another event in a regular way. Hence if 
free decisions are jettisoned, a terminology alternative to causal language would suffice, probably one based on the concept of statistical correlations.
</span></p>
<p class=PGStandard style='text-align:justify'><span lang=EN-GB>
The most generic way to define a causal effect mathematically is to consider it as a difference-maker for the probability of a specific variant of an event
happening. The different variants for discrete cases are different values, e.g. A=a, for an event-variable A. 
The starting point is to consider statistical dependence: P(B=b|A=a), the probability of B=b given A=a. If the leftmost of the three graphs above is considered, 
and if we think of
observationally gathered data of the joint probability P(A=a,B=b), or short P(a,b) then 
</span></p>
<p>
    \begin{equation}
    P(a,b) = P(a|b)P(b) = P(b|a)P(a) \\
    \text{Therefore:} \\
    P(b|a) = \frac{P(a,b)}{P(a)}  = \frac{P(a,b)}{\sum_{b'} P(a,b')}  \\
    \end{equation}
</p>
<p class=PGStandard style='text-align:justify'><span lang=EN-GB>
This is the probability of observing b given one also observes a. In contrast to observational data, interventional data, following Judea Pearl represented as
P(b| set(a)) or alternatively P(b| do(a)), will in most causal scenarios be calculated differently, as can be seen in the example of the three variable case below. Alternatively, it can
be directly measured by producing the interventional data by setting A to a designated value a. In the leftmost of the three graphs above, P(b|a) = P(b|do(a)). If a causes
b and both have no shared causes, it does not matter for P(b) whether I observe A=a or make sure that A=a. In the middle graph, P(b|a) <> P(b|do(a)). Given
I observe A=a, the chance of also observing b increases since it is probable that b has in fact caused a. But setting A=a does not increase b's chance
of occurring, because A has no influence on B. 
</span></p>
<p class=PGStandard style='text-align:justify'><span lang=EN-GB>
What is the difference between P(b|a) = P(b|do(a)) in a three variable case where we have an explicit shared cause of A and B and an additional causal 
influence of A on B?
</span></p>
<p align="center"><img src="img/three_variables.png" style="max-width:15%;"/></p>
<p class=PGStandard style='text-align:justify'><span lang=EN-GB>
Again it is true that:
</span></p>
<p>
    \begin{equation}
    P(b|a) = \frac{P(a,b)}{P(a)} \\
    \end{equation}
</p>
<p class=PGStandard style='text-align:justify'><span lang=EN-GB>
which can be resolved, if we start from P(a,b,c) as the known distribution, into:
</span></p>
<p>
    \begin{equation}
    P(b|a) = \sum_{c'} P(b|a,c') = \frac{\sum_{c'} P(a,b,c')}{\sum_{b',c'} P(a,b',c')} \\
    \end{equation}
</p>
<p class=PGStandard style='text-align:justify'><span lang=EN-GB>
In contrast, P(b|set(a)) is now:
</span></p>
<p>
    \begin{equation}
    P(b|set(a)) = \sum_{c'} P(c') P(b|a,c') \\
    \end{equation}
</p>
<p class=PGStandard style='text-align:justify'><span lang=EN-GB>
Like for P(b|a), the probability of observing b depends on observing a while taking into account
all cases of different C's as well. But now it is necessary to multiply the conditional probability
P(b|a,c') with the marginal probability P(c'), since if we set A = a, the marginal probability of the
different C's will be preserved - which is not the case if we condition on P(b|a) observationally.
Thus the cases of the different values of C will co-occur with the intervention as dictated
by their marginal probability and their contribution to the observed P(b|set(a)) will have to 
be weighed according to P(c').
</span></p>
</div>

</body>

</html>
